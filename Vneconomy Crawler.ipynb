{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ddb884e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4dc674",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:46.0) \"\n",
    "                  \"Gecko/20100101 Firefox/46.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3d3b38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_link(link):\n",
    "    link_list = []\n",
    "    req = requests.Session().get(link, headers=HEADERS).text\n",
    "    soup = BeautifulSoup(req, \"lxml\")\n",
    "    body_page = soup.find('div', class_=\"col-12 col-lg-9 column-border\")\n",
    "    links = body_page.find_all('figure', class_=\"story__thumb\")\n",
    "    for i in links:\n",
    "        link_list.append(i.find('a')['href'])\n",
    "\n",
    "    return link_list\n",
    "\n",
    "\n",
    "def get_highlight_link(link):\n",
    "    link_list = []\n",
    "    req = requests.Session().get(link, headers=HEADERS).text\n",
    "    soup = BeautifulSoup(req, \"lxml\")\n",
    "    story_highlight1 = soup.find('div', class_=\"col-12 col-lg-6\")\n",
    "    story_highlight2 = soup.find('div', class_=\"col-12 col-md-6 col-lg-3\")\n",
    "\n",
    "    links1 = story_highlight1.find_all('figure', class_=\"story__thumb\")\n",
    "    for i in links1:\n",
    "        link_list.append(i.find('a')['href'])\n",
    "\n",
    "    links2 = story_highlight2.find_all('figure', class_=\"story__thumb\")\n",
    "    for i in links2:\n",
    "        link_list.append(i.find('a')['href'])\n",
    "\n",
    "    return link_list\n",
    "\n",
    "\n",
    "def page_is_empty(link):\n",
    "    req = requests.Session().get(link, headers=HEADERS).text\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    body_page = soup.find('div', class_=\"col-12 col-lg-9 column-border\")\n",
    "    articles = body_page.find_all('article', class_=\"story story--featured story--timeline\")\n",
    "    if len(articles) > 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def get_categories(link):\n",
    "    catergories = []\n",
    "    req = requests.Session().get(link, headers=HEADERS).text\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    bar1 = soup.find_all('a', class_=\"link__extend\")\n",
    "    for i in bar1[1:4]:\n",
    "        catergories.append(i['href'])\n",
    "\n",
    "    bar2 = soup.find_all('a', class_=\"nav-link\")\n",
    "    for i in bar2:\n",
    "        if 'htm' in str(i['href']):\n",
    "            catergories.append(i['href'])\n",
    "\n",
    "    return catergories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce8d5b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def crawl(link):\n",
    "    req = requests.Session().get(link, headers=HEADERS).text\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "\n",
    "    try:\n",
    "        menu = soup.find('h1', class_=\"category-main\").text\n",
    "    except:\n",
    "        menu = ''\n",
    "\n",
    "    try:\n",
    "        tieude = soup.find('h1', class_=\"detail__title\").text\n",
    "    except:\n",
    "        tieude = ''\n",
    "\n",
    "    try:\n",
    "        anh = soup.find('figure', class_=\"detail__avatar\").find('img')['src']\n",
    "    except:\n",
    "        anh = ''\n",
    "\n",
    "    try:\n",
    "        time_web = soup.find('div', class_=\"detail__meta\").text\n",
    "    except:\n",
    "        time_web = ''\n",
    "\n",
    "    try:\n",
    "        publish_time = datetime.strptime(time_web, '%H:%M %d/%m/%Y').timestamp()\n",
    "    except:\n",
    "        publish_time = ''\n",
    "\n",
    "    try:\n",
    "        motangan = soup.find('h2', class_=\"detail__summary\").text\n",
    "    except:\n",
    "        motangan = ''\n",
    "\n",
    "    try:\n",
    "        body = soup.find('div', class_=\"detail__content\").text\n",
    "    except:\n",
    "        body = ''\n",
    "\n",
    "\n",
    "    dictionnary = {\n",
    "        'Menu': menu,\n",
    "        'Tieude': tieude,\n",
    "        'Anh': anh,\n",
    "        'Lienket': link,\n",
    "        'Motangan': motangan,\n",
    "        'Body': body,\n",
    "        'Publish_time': publish_time,\n",
    "        'Received_time': datetime.now().timestamp()\n",
    "    }\n",
    "\n",
    "    return dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vneconomy_categories = get_categories('https://vneconomy.vn/')\n",
    "\n",
    "for i in vneconomy_categories:\n",
    "    print(i)\n",
    "    highlights = get_highlight_link('https://vneconomy.vn/' + i)\n",
    "    for k in highlights:\n",
    "        if not ('epaper' in k or 'files' in k):\n",
    "            try:\n",
    "                content = crawl('https://vneconomy.vn' + k)\n",
    "                if float(content['Publish_time']) < float(time.mktime(datetime.now().date().timetuple())):\n",
    "                    break\n",
    "                else:\n",
    "                    json_object = json.dumps(content, indent=4)\n",
    "                    g = open('/home/khoatm4/Elastic_search/News/News_folder/' + 'vneconomy_' + k[1:] + '.json', 'w+')\n",
    "                    g.write(json_object)\n",
    "            except:\n",
    "                print('https://vneconomy.vn' + k)\n",
    "\n",
    "    j = 1\n",
    "    while not page_is_empty():\n",
    "        links = get_link('https://vneconomy.vn/' + i + '?trang=' + str(j))\n",
    "        for l in links:\n",
    "            if not ('epaper' in l or 'files' in l):\n",
    "                try:\n",
    "                    content = crawl('https://vneconomy.vn' + l)\n",
    "                    json_object = json.dumps(content, indent=4)\n",
    "                    g = open('/home/khoatm4/Elastic_search/News/News_folder/' + 'vneconomy_' + l[1:] + '.json', 'w+')\n",
    "                    g.write(json_object)\n",
    "\n",
    "                except:\n",
    "                    print('https://vneconomy.vn' + l)\n",
    "\n",
    "        print(j)\n",
    "        j += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c9304",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}